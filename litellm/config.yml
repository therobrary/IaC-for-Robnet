model_list:
  # Ollama Models - Local inference server at 192.168.6.5:11434
  - model_name: magistral
    litellm_params:
      model: ollama/magistral:latest
      api_base: http://192.168.6.5:11434

  - model_name: mistral-small3.2
    litellm_params:
      model: ollama/mistral-small3.2:latest
      api_base: http://192.168.6.5:11434

  # OpenRouter Models - Free tier models via OpenRouter API
  - model_name: OR-DeepSeek-R1-0528
    litellm_params:
      model: openrouter/deepseek/deepseek-r1-0528:free
      api_base: https://openrouter.ai/api/v1
      api_key: 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
    model_info:
      supports_web_search: true
      supports_function_calling: true
      supports_response_schema: true
      supports_tool_choice: true
      supports_parallel_function_calling: true

  - model_name: OR-DeepSeek-V3
    litellm_params:
      model: openrouter/deepseek/deepseek-chat-v3-0324:free
      api_base: https://openrouter.ai/api/v1
      api_key: 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
    model_info:
      supports_web_search: true
      supports_function_calling: true
      supports_response_schema: true
      supports_tool_choice: true
      supports_parallel_function_calling: true

  # Mistral Models - Standard mistral-small-latest load balanced across API keys
  - model_name: mistral-small-latest
    litellm_params:
      model: mistral/mistral-small-latest
      api_key: YCIZcE6XJefvNDshEDjveIcoE9237Ef5
      rpm: 60      # Requests per minute limit
      tpm: 500000  # Tokens per minute limit
    model_info:
      id: "mistral-1"

  - model_name: mistral-small-latest
    litellm_params:
      model: mistral/mistral-small-latest
      api_key: nQCpJNxluldnzXd6ylhOJiaA1j0zlXCV
      rpm: 60
      tpm: 500000
    model_info:
      id: "mistral-2"

  # Mistral Models - Standard mistral-medium-latest load balanced across API keys
  - model_name: mistral-medium-latest
    litellm_params:
      model: mistral/mistral-medium-latest
      api_key: YCIZcE6XJefvNDshEDjveIcoE9237Ef5
      rpm: 60
      tpm: 500000
    model_info:
      id: "mistral-medium-1"

  - model_name: mistral-medium-latest
    litellm_params:
      model: mistral/mistral-medium-latest
      api_key: nQCpJNxluldnzXd6ylhOJiaA1j0zlXCV
      rpm: 60
      tpm: 500000
    model_info:
      id: "mistral-medium-2"

  # Mistral Models - Standard mistral-large-latest load balanced across API keys
  - model_name: mistral-large-latest
    litellm_params:
      model: mistral/mistral-large-latest
      api_key: YCIZcE6XJefvNDshEDjveIcoE9237Ef5
      rpm: 60
      tpm: 500000
    model_info:
      id: "mistral-large-1"

  - model_name: mistral-large-latest
    litellm_params:
      model: mistral/mistral-large-latest
      api_key: nQCpJNxluldnzXd6ylhOJiaA1j0zlXCV
      rpm: 60
      tpm: 500000
    model_info:
      id: "mistral-large-2"

  # Mistral Models - Magistral small variant load balanced across API keys
  - model_name: magistral-small-latest
    litellm_params:
      model: mistral/magistral-small-latest
      api_key: YCIZcE6XJefvNDshEDjveIcoE9237Ef5
      rpm: 60
      tpm: 500000
    model_info:
      id: "magistral-1"

  - model_name: magistral-small-latest
    litellm_params:
      model: mistral/magistral-small-latest
      api_key: nQCpJNxluldnzXd6ylhOJiaA1j0zlXCV
      rpm: 60
      tpm: 500000
    model_info:
      id: "magistral-2"

  # Mistral Models - Magistral medium variant load balanced across API keys
  - model_name: magistral-medium-latest
    litellm_params:
      model: mistral/magistral-medium-latest
      api_key: YCIZcE6XJefvNDshEDjveIcoE9237Ef5
      rpm: 60
      tpm: 500000
    model_info:
      id: "magistral-medium-1"

  - model_name: magistral-medium-latest
    litellm_params:
      model: mistral/magistral-medium-latest
      api_key: nQCpJNxluldnzXd6ylhOJiaA1j0zlXCV
      rpm: 60
      tpm: 500000
    model_info:
      id: "magistral-medium-2"

  # Meta Llama Models - Single entries
  - model_name: llama-maverick-17b
    litellm_params:
      model: meta_llama/Llama-4-Maverick-17B-128E-Instruct-FP8
      api_key: YOUR_LLAMA_API_KEY  # Replace with actual API key
      rpm: 60
      tpm: 500000
    model_info:
      supports_function_calling: true

  - model_name: llama-scout-8b
    litellm_params:
      model: meta_llama/Llama-4-Scout-8B-128E-Instruct-FP8
      api_key: YOUR_LLAMA_API_KEY  # Replace with actual API key
      rpm: 60
      tpm: 500000
    model_info:
      supports_function_calling: true

  # Together AI Models
  - model_name: together-llama-vision
    litellm_params:
      model: together_ai/meta-llama/Llama-Vision-Free
      api_key: YOUR_TOGETHERAI_API_KEY  # Replace with actual API key
      rpm: 60
      tpm: 60000  # Updated to 60,000 tokens per minute
    model_info:
      supports_vision: true
      supports_function_calling: true

# Fallback configuration
fallbacks:
  - "mistral-small-latest": ["mistral-small3.2"]
  - "magistral-small-latest": ["magistral"]

# Router settings for load balancing and performance optimization
router_settings:
  routing_strategy: simple-shuffle  # Distribute requests across deployments
  num_retries: 3  # Retry failed requests up to 3 times
  timeout: 30  # Request timeout in seconds

# Global LiteLLM settings
litellm_settings:
  drop_params: true  # Drop unsupported parameters instead of erroring
  set_verbose: false  # Disable verbose logging
